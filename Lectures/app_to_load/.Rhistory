plot(subset(x = coal_data, select = c("Operating.Year.avg", "Nameplate.Capacity", "Energy.Source.1", "retire")))
## Energy.Source.1 has been converted to a number for plot
levels(coal_data$Energy.Source.1)[1]
coal_train <- coal_data[1:450,]
coal_test <- coal_data[451:597,]
## build the logit model
lgm <- glm(retire ~ Nameplate.Capacity + Operating.Year.avg + Energy.Source.1, data = coal_train, family = binomial(link = "logit"))
summary(lgm)
##
anova(lgm, test="Chisq")
pR2(lgm)
contrasts(coal_data$Energy.Source.1)
fitted.results <- predict(lgm, newdata=subset(coal_test,select=c('Nameplate.Capacity', 'Operating.Year.avg', 'Energy.Source.1')),type='response')
fitted.results
fitted.results <- ifelse(fitted.results > 0.5,1,0)
fitted.results
misClasificError <- mean(fitted.results != coal_test$retire)
print(paste('Accuracy ',round(1-misClasificError, 4)*100, '%', sep = ''))
summary(lgm)
1/(1 + exp(-(2.071e+02 + -1.486e-03*500 + -1.051e-01*1980 + -1.995e+00)))
1/(1 + exp(-(2.071e+02 + -1.486e-03*750 + -1.051e-01*1960 + -1.995e+00)))
## probibility of a 500 MW SUB coal plant built in 1980 being retired
1/(1 + exp(-(2.071e+02 + -1.486e-03*500 + -1.051e-01*1980 + -4.134e-01)))
## probibility of a 750 MW SUB coal plant built in 1960 being retired
1/(1 + exp(-(2.071e+02 + -1.486e-03*750 + -1.051e-01*1960 + -4.134e-01)))
## probibility of a 500 MW BIT coal plant built in 1980 being retired
1/(1 + exp(-(2.071e+02 + -1.486e-03*500 + -1.051e-01*1980 )))
## probibility of a 500 MW BIT coal plant built in 1980 being retired
1/(1 + exp(-(2.071e+02 + -1.486e-03*500 + -1.051e-01*1970 )))
p <- predict(lgm, newdata=subset(coal_test,select=c('Nameplate.Capacity', 'Operating.Year.avg', 'Energy.Source.1')),type='response')
pr <- prediction(p, coal_test$retire)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
library(ROCR)
p <- predict(lgm, newdata=subset(coal_test,select=c('Nameplate.Capacity', 'Operating.Year.avg', 'Energy.Source.1')),type='response')
pr <- prediction(p, coal_test$retire)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
abline(a=0,b=1,col='red')
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
## one more check on the goodness of fit
step <- stepAIC(lgm, direction="both")
step$anova
## load 'em up
library(fpc)
install.packages("fpc")
## load 'em up
library(fpc)
library(cluster)
library(NbClust)
install.packages(c("NbClust", "HSAUR", "factoextra", "dplyr", "dbscan", "ape"))
## load 'em up
library(fpc)
library(cluster)
library(NbClust)
library(HSAUR)
library(factoextra)
library(dplyr)
library(dbscan)
library(ape)
## clustering data to make up
set.seed(1)
## generate some made-up data that is pretty closely clustered
## rnorm give you n random samples from a normal distribution
x1 <- rnorm(n = 50, mean = 3)
y1 <- rnorm(n = 50, mean = 3)
x2 <- rnorm(n = 50, mean = 6)
y2 <- rnorm(n = 50, mean = 5)
x3 <- rnorm(n = 50, mean = 7)
y3 <- rnorm(n = 50, mean = 1)
## bind the origional data together and give it some color
v1 <- as.data.frame(cbind(x1, y1))
v1$col <- 'red'
v2 <- as.data.frame(cbind(x2, y2))
names(v2) <- c('x1', 'y1')
v2$col <- 'blue'
v3 <- as.data.frame(cbind(x3, y3))
names(v3) <- c('x1', 'y1')
v3$col <- 'green'
## merge all the data together
raw_data <- as.data.frame(rbind(v1, v2, v3))
## See 10 random rows
sample_n(raw_data, 10)
## plot the data
plot(y1 ~ x1, data = raw_data, pch = 16, col = col)
## scale the data, scales around 0
raw_data_scaled <- raw_data
raw_data_scaled$x1 <- scale(raw_data_scaled$x1)
raw_data_scaled$y1 <- scale(raw_data_scaled$y1)
## See 10 random rows
sample_n(raw_data_scaled, 10)
## plot the data
plot(y1 ~ x1, data = raw_data_scaled, pch = 16, col = col)
## this is a test to help determine how many clusters that our data might have
nc <- NbClust(data = raw_data_scaled[,1:2], method = 'kmeans')
## Warning: just becuase this test tells us that 3 is the best, we should make sure that makes since given our understaing of the data
## we now perform the k-means clustering with the 3 specified clusters
k1 <- kmeans(x = raw_data_scaled[,1:2], iter.max = 100, centers = 3)
## see output
k1
## merge together the origional data with the selected cluster
data_km <- cbind(raw_data_scaled, k1$cluster)
names(data_km) <- c('x1', 'y1', 'col', 'clus')
data_km$col2 <- 'blue'
data_km$col2[data_km$clus == 2] <- 'red'
data_km$col2[data_km$clus == 1] <- 'green'
## plot both the origional data sent to the clustering and the post clustered data
plot(y1 ~ x1, data = data_km, pch = 16, col = col, main = 'Original groups')
plot(y1 ~ x1, data = data_km, pch = 16, col = col2, main = 'Clustered goups')
## see what data can be extracted from k1 (also lm, etc -- structured output)
k1[]
## extract the k-means centers
k1$centers
## plot clusters and cluster centers
plot(y1 ~ x1, data = data_km, pch = 16, col = col2, main = 'Clustered goups, kmeans')
points(k1$centers[1,1], k1$centers[1,2], col = 'black', pch = 13, cex = 2)
points(k1$centers[2,1], k1$centers[2,2], col = 'black', pch = 13, cex = 2)
points(k1$centers[3,1], k1$centers[3,2], col = 'black', pch = 13, cex = 2)
## density based clustering using dbscan, same data
db1 <- dbscan(x = raw_data_scaled[,1:2], eps = 0.3, minPts = 4)
db1
data_db <- cbind(raw_data_scaled, db1$cluster)
## name data and get colors of clusters and noise points (clus == 0)
names(data_db) <- c('x1', 'y1', 'col', 'clus')
data_db$col2[data_db$clus == 3] <- 'blue'
data_db$col2[data_db$clus == 2] <- 'red'
data_db$col2[data_db$clus == 1] <- 'green'
data_db$col2[data_db$clus == 0] <- 'gray'
plot(y1 ~ x1, data = data_db, pch = 16, col = col2, main = 'Clustered goups, dbscan')
## clustering lines (24-dimensional vector) with kmeans
## read in some time series data, actual home energy use profiles for 111 homes in Austin, TX
profiles_data <- read.csv('FINAL_summer_profiles.csv')
# set the dataid as the row.name so that we can keep track of it later
row.names(profiles_data) <- profiles_data$X
# since the dataid is now the row.name, we can drop it
profiles_data <- profiles_data[,2:25]
head(profiles_data)
# plot the origional data
matplot(t(profiles_data), type = "l", col = 'black', lty = 1)
## this is a test to help determine how many clusters that our data might have
NbClust(profiles_data, method = 'kmeans')
# do the clustering analysis on the data itself
profiles_clust <- kmeans(x = profiles_data, centers = 2, nstart = 5)
# merge tegether the origional data with the new cluster number
p2 <- cbind(profiles_data, profiles_clust$cluster)
# I just want to rename the last column
names(p2) <- c(names(p2[1:24]), 'cluster')
## reset graphcis paramenters
par(mfrow=c(1,1))
## plot the origional data as well as the new cluster "center"
matplot(t(p2[p2$cluster == 1,1:24]), type = "l", col = 'orange', lty = 2)
matplot(profiles_clust$centers[1,], type = "l", col = 'orange', lty = 1, lwd = 5, add = T)
matplot(t(p2[p2$cluster == 2,1:24]), type = "l", col = 'blue', lty = 2, add = T)
matplot(profiles_clust$centers[2,], type = "l", col = 'blue', lty = 1, lwd = 5, add = T)
## Hierarchical clustering
## read in 2016 EIA data, subset down to ERCOT (TX) power plants (NERC.Region = TRE)
tx <- read.csv('tx_plants.csv', stringsAsFactors = F)
## limit the data to just NGCC and coal plants
tx <- tx[tx$tech %in% c('Natural Gas Fired Combined Cycle', 'Conventional Steam Coal'),]
tx$tech[tx$tech == 'Natural Gas Fired Combined Cycle'] <- 'NGCC'
tx$tech[tx$tech == 'Conventional Steam Coal'] <- 'Coal'
tx <- tx[tx$heat_rate < 20000,]
tx$col <- 'blue'
tx$col[tx$tech == 'Coal'] <- 'orange'
head(tx)
## cluster on columns 3 - 5 (plant size, capacity factor, and heatrate)
clusters <- hclust(dist(tx[, 3:5]))
## cut the tree at two clusers and see how good it was at predicting
clusterCut <- cutree(clusters, 2)
table(clusterCut, tx$tech)
## plot the tree
plot(as.phylo(clusters), cex = 0.5, tip.color = tx$col, label.offset = 1)
## install keras and tensorflow
install.packages("keras")
library(keras)
install_keras()
setwd("~/Desktop/Adv_Vis/Lectures")
library(ROCR)
library(pscl)
## initialize the random number generator
set.seed(2)
## read in data
coal_data <- read.csv('COAL_PLANTS.csv')
## create training and test datasets
coal_train <- coal_data[1:450,]
coal_test <- coal_data[451:597,]
## create model
lgm <- glm(retire ~ Nameplate.Capacity + Operating.Year.avg + Energy.Source.1, data = coal_train, family = binomial(link = "logit"))
## look at the output
summary(lgm)
## check the results
fitted.results <- predict(lgm, newdata=subset(coal_test,select=c('Nameplate.Capacity', 'Operating.Year.avg', 'Energy.Source.1')),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != coal_test$retire)
print(paste('Accuracy Logit: ',round(1-misClasificError, 4)*100, '%', sep = ''))
## neural networks -- this will use the same problem as the logit example, but will use a neural network instead
library(caTools)
library(neuralnet)
## read in data and downselect to just the needed columns
coal_data <- read.csv('COAL_PLANTS.csv')
coal2 <- coal_data[c('retire', 'Nameplate.Capacity', 'Operating.Year.avg', 'Energy.Source.1')]
## neural networks often need to be normalized to converge, find the max and min for each column
maxs <- apply(coal2[,2:3], 2, max)
mins <- apply(coal2[,2:3], 2, min)
## this will scale the continious varaibles (columns 2 & 3) from 0 to 1
scaled.data <- as.data.frame(scale(coal2[,2:3], center = mins, scale = maxs - mins))
## add back in the retire and Energy.Source.1 columns & rename
data <- cbind(coal2$retire, scaled.data, coal2$Energy.Source.1)
names(data) <- c('retire', 'Nameplate.Capacity', 'Operating.Year.avg', 'Energy.Source.1')
head(data)
## since NN need numeric values, transform the factor variable 'Energy.Source.1' to a list of binaries, note "BIT" is the default like last time
exp_data <- as.data.frame(model.matrix( ~ Nameplate.Capacity + Operating.Year.avg + Energy.Source.1, data = data))
head(exp_data)
## combine 1) all the columns from data, except the last one with 2) all the columns from exp_data except the first 3
data <- cbind(data[,-dim(data)[2]], exp_data[,-c(1:3)])
## set seed to whatever, used in the random number generator, if you change this, your answer might vary from mine
set.seed(2505)
## just another way to split a dataset into a training and test dataset
split = sample.split(data$retire, SplitRatio = 0.70)
head(split)
train = subset(data, split == TRUE)
test = subset(data, split == FALSE)
## auto-generate the formula from the names of the dataset -- this is not necessary, but nice if you have a bunch of columns
feats <- names(train[,-1])
f <- paste(feats,collapse=' + ')
f <- paste('retire ~',f)
f <- as.formula(f)
## build the neural net using the training dataset and specify the number of hidden nodes, default: hidden = c(1)
nn <- neuralnet(f, train, linear.output=T, hidden = c(3,2))
## use the neural network just created, named "nn" to try to predict the retirement status of the test dataset
predicted.nn.values <- compute(nn,test[2:7])
## build a table to see how we did in predicting
table(test$retire,predicted.nn.values$net.result)
## test the accuracy % wise
print(paste('Accuracy NN: ',round((table(test$retire,predicted.nn.values$net.result)[1,1] + table(test$retire,predicted.nn.values$net.result)[2,2])/dim(test)[1], 4)*100, '%', sep = ''))
## plot the neural network
plot(nn)
t1 <- test[3,2:7]
compute(nn,t1)
#The MNIST dataset is included with Keras and can be accessed using the dataset_mnist() function. Here we load the dataset then create variables for our test and training data:
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
#The x data is a 3-d array (images,width,height) of grayscale values. To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:
dim(x_train)
## rescale the image to be a vector of pixels
dim(x_train) <- c(nrow(x_train), 784)
dim(x_test) <- c(nrow(x_test), 784)
dim(x_train)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
#The y data is an integer vector with values ranging from 0 to 9. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras to_categorical() function:
# similair to the way we categorized types of coal plants with binaries, in fact, exactly like that
y_train[1:20] # y_test is similiar
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
y_train[1:20,1:10]
summary(y_train) #shows how often a digit shows up
#The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the sequential model, a linear stack of layers.
#We begin by creating a sequential model and then adding layers using the pipe (%>%) operator:
# In the keras NN we have to "manually" build the layers
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = "relu", input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = "softmax")
# activation = "relu" : https://www.tensorflow.org/api_guides/python/nn#Activation_Functions
# Here softmax is serving as an “activation” or “link” function, shaping the output of our linear function into the form we want – in this case, a probability distribution over 10 cases. You can think of it as converting tallies of evidence into probabilities of our input being in each class. It’s defined as:
?layer_dense
summary(model)
# Next, compile the model with appropriate loss function, optimizer, and metrics:
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(),
metrics = c("accuracy")
)
# Next, compile the model with appropriate loss function, optimizer, and metrics:
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_rmsprop(),
metrics = c("accuracy")
)
#Use the fit() function to train the model for 30 epochs using batches of 128 images:
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
plot(history)
model %>% evaluate(x_test, y_test,verbose = 0)
# the loss is usually negative log-likelihood and residual sum of squares for classification and regression respectively
library(lpSolveAPI)
#define the datasets
crops<-data.frame(type=c('wheat','barley'), acre_cost=c(120,210), harvest_bushels_per_acre=c(110,30), harvest_profit_per_bushel=c(1.3, 2.0))
#define the right hand side of the constraints
money <- 15000
harvest_storage_bushels <- 4000
plant_acres <- 75
#create an LP model with 3 constraints and 2 decision variables
lpmodel<-make.lp(3,2)
#fill in the coefficients of the constraints. You could also use "set.row" or other techniques
set.column(lpmodel,1,c(crops$acre_cost[1], crops$harvest_bushels_per_acre[1], 1),c(1,2,3))
set.column(lpmodel,2,c(crops$acre_cost[2], crops$harvest_bushels_per_acre[2], 1),c(1,2,3))
#view the model
lpmodel
#set objective coefficients
set.objfn(lpmodel, c(crops$harvest_bushels_per_acre[1]*crops$harvest_profit_per_bushel[1], crops$harvest_bushels_per_acre[2]*crops$harvest_profit_per_bushel[2]))
#set constraints
set.rhs(lpmodel, c(money, harvest_storage_bushels, plant_acres))
set.constr.type(lpmodel, c("<=", "<=", "<="))
#set objective direction
lp.control(lpmodel,sense='max')
#we can write the model to a text file to visualize how the model object is being written and make sure it aligns with our mathematical equations. This file can be viewed in SublimeText or other readers.
#you can also just run "lpmodel" (or whatever the name of your linear program object is) and the console will print table of coefficients/information
#however, for larger models, the console will not print everything, and using "write.lp" is the best method for viewing the linear program object
write.lp(lpmodel,'example_1.lp',type='lp')
#solve the model, if this return 0 an optimal solution is found
solve(lpmodel)
#this returns the proposed solution
get.objective(lpmodel) #value of the objective (i.e. the farmer's profit)
get.variables(lpmodel) #value of the variables for achieving the objective (how much wheat and barley to plant)
get.constraints(lpmodel) #value of the constraint equations. If get.constraints = the right hand side of the constraint equation, then that constraint is "binding" and the solution could be better if we could improve that constraint. For example, the money constraint in the solution is $13,781.25 which is less than the $15,000 of money the farmer starts with. So giving the farmer more money up front would not enable them to make more profit. However, the storage constraint is 4,000 which is equal to the farmer's 4000 bushel storage capacity. So giving the farmer more storage capacity might enable them to increase profit.
library(lpSolveAPI)
#define the datasets
train<-data.frame(wagon=c('w1','w2','w3'), weightcapacity=c(10,8,12), spacecapacity=c(5000,4000,8000))
cargo<-data.frame(type=c('c1','c2','c3','c4'), available=c(18,10,5,20), volume=c(400,300,200,500),profit=c(2000,2500,5000,3500))
#create an LP model with 10 constraints and 12 decision variables
lpmodel<-make.lp(2*NROW(train)+NROW(cargo), NROW(train) * NROW(cargo))
#write the lp to a file that we can view in SublimeText or other readers
write.lp(lpmodel,'example_2.lp',type='lp')
#I used this to keep count within the loops, I admit that this could be done a lot neater
column<-0
row<-0
#build the model column per column
for(wg in train$wagon){
row<-row+1 #tracks our loop progress
for(type in seq(1,NROW(cargo$type))){
column<-column+1 #tracks our loop progress
#this takes the arguments 'column','values' & 'indices' (as in where these values should be placed in the column)
set.column(lpmodel,column,c(1, cargo[type,'volume'],1), indices=c(row,NROW(train)+row, NROW(train)*2+type))
}}
write.lp(lpmodel,'example_2.lp',type='lp') #see how the linear program looks now
#set rhs constraints for weight capacities
set.rhs(lpmodel, train$weightcapacity, constraints=seq(1,NROW(train)))
write.lp(lpmodel,'example_2.lp',type='lp') #see how the linear program looks now
#set rhs constraints for space capacities
set.rhs(lpmodel, train$spacecapacity, constraints=seq(NROW(train)+1,NROW(train)*2))
write.lp(lpmodel,'example_2.lp',type='lp') #see how the linear program looks now
#set rhs constraints for amount of available cargo
set.rhs(lpmodel, cargo$available, constraints=seq(NROW(train)*2+1,NROW(train)*2+NROW(cargo)))
write.lp(lpmodel,'example_2.lp',type='lp') #see how the linear program looks now
#set objective coefficients
set.objfn(lpmodel, rep(cargo$profit,NROW(train)))
#set objective direction
lp.control(lpmodel,sense='max')
write.lp(lpmodel,'example_2.lp',type='lp') #see how the linear program looks now. We should be done at this point.
#solve the model, if this returns 0 an optimal solution is found
solve(lpmodel)
#this returns the proposed solution
get.objective(lpmodel)
get.variables(lpmodel)
get.constraints(lpmodel)
#we can access the solutions piecemeal to help us keep track of what information is in the solution. This requires some organization, so be careful.
#for example
variable_results = get.variables(lpmodel)
loop_num = 0
for (wag in train$wagon){
for (crg in cargo$type){
loop_num <- loop_num+1
print (paste("Cargo #", crg, " in Wagon #", wag, ": ", variable_results[loop_num], sep=""))
}
}
#which could be organized into a dataframe or whatever results output you want
knitr::opts_chunk$set(echo = TRUE)
library(shiny)
library(rsconnect)
install.packages("rsconnect")
library(shiny)
library(rsconnect)
# Define UI for miles per gallon app ----
ui <- pageWithSidebar(
# App title ----
headerPanel("Miles Per Gallon"),
# Sidebar panel for inputs ----
sidebarPanel(),
# Main panel for displaying outputs ----
mainPanel()
)
# Define server logic to plot various variables against mpg ----
server <- function(input, output) {
}
shinyApp(ui, server)
library(shiny)
library(rsconnect)
ui <- pageWithSidebar(
headerPanel('Iris k-means clustering'),
sidebarPanel(
selectInput('xcol', 'X Variable', names(iris)),
selectInput('ycol', 'Y Variable', names(iris),
selected=names(iris)[[2]]),
numericInput('clusters', 'Cluster count', 3,
min = 1, max = 9)
),
mainPanel(
plotOutput('plot1')
)
)
server <- function(input, output, session) {
# Combine the selected variables into a new data frame
selectedData <- reactive({
iris[, c(input$xcol, input$ycol)]
})
clusters <- reactive({
kmeans(selectedData(), input$clusters)
})
output$plot1 <- renderPlot({
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3",
"#FF7F00", "#FFFF33", "#A65628", "#F781BF", "#999999"))
par(mar = c(5.1, 4.1, 0, 1))
plot(selectedData(),
col = clusters()$cluster,
pch = 20, cex = 3)
points(clusters()$centers, pch = 4, cex = 4, lwd = 4)
})
}
shinyApp(ui, server)
library(shiny)
library(rsconnect)
library(sp)
library(rgdal)
library(raster)
library(colorRamps)
library(rasterVis)
library(rgeos)
## data to keep with the app
county <- shapefile('US_COUNTY_SHPFILE/US_county_cont.shp')
ui <- pageWithSidebar(
headerPanel('Plot a state'),
sidebarPanel(
selectInput('state', 'Select State', sort(unique(county$STATE_NAME))),
selectInput('incl_county', 'Include counties?', c('Yes','No')),
textInput('fill_col', 'Fill color?', 'lightgray'),
textInput('bor_col', 'Border color?', 'black')
),
mainPanel(
plotOutput('plot1')
)
)
server <- function(input, output) {
# get the state name from the ui section and subset the county data to just that state
state_data <- reactive({
# if the user did not want to include counties, merge all the counties together
if (input$incl_county == 'No') {
s1 <- county[county$STATE_NAME == input$state,]
gUnaryUnion(s1, id = s1@data$STATE_NAME)
# if user wanted counties, leave them as-is
}
else { county[county$STATE_NAME == input$state,] }
})
output$plot1 <- renderPlot({
plot(state_data(), col = input$fill_col, border = input$bor_col) ## for some reason, state_data needs a () after it
})
}
shinyApp(ui, server)
library(shiny)
library(rsconnect)
library(sp)
library(rgdal)
library(raster)
library(colorRamps)
library(rasterVis)
library(rgeos)
## data to keep with the app
county <- shapefile('US_COUNTY_SHPFILE/US_county_cont.shp')
ui <- pageWithSidebar(
headerPanel('Plot a state'),
sidebarPanel(
selectInput('state', 'Select State', sort(unique(county$STATE_NAME))),
selectInput('incl_county', 'Include counties?', c('Yes','No')),
textInput('fill_col', 'Fill color?', 'lightgray'),
textInput('bor_col', 'Border color?', 'black')
),
mainPanel(
plotOutput('plot1')
)
)
server <- function(input, output) {
# get the state name from the ui section and subset the county data to just that state
state_data <- reactive({
# if the user did not want to include counties, merge all the counties together
if (input$incl_county == 'No') {
s1 <- county[county$STATE_NAME == input$state,]
gUnaryUnion(s1, id = s1@data$STATE_NAME)
# if user wanted counties, leave them as-is
}
else { county[county$STATE_NAME == input$state,] }
})
output$plot1 <- renderPlot({
plot(state_data(), col = input$fill_col, border = input$bor_col) ## for some reason, state_data needs a () after it
})
}
shinyApp(ui, server)
runApp('app_to_load')
runApp('app3.R')
runApp('app_to_load')
